{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn.metrics as sklm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>Country</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Extraversion</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>Impulsive</th>\n",
       "      <th>Sensation Seeking</th>\n",
       "      <th>Benzos</th>\n",
       "      <th>Caffeine</th>\n",
       "      <th>Cocaine</th>\n",
       "      <th>LSD</th>\n",
       "      <th>Heroin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>0.12600</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>-0.21712</td>\n",
       "      <td>-1.18084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.07854</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>-0.71126</td>\n",
       "      <td>-0.21575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.49788</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>-1.37983</td>\n",
       "      <td>0.40148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.16365</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>-1.37983</td>\n",
       "      <td>-1.18084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>-0.21712</td>\n",
       "      <td>-0.21575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-1.19430</td>\n",
       "      <td>1.74091</td>\n",
       "      <td>1.88511</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-1.13788</td>\n",
       "      <td>0.88113</td>\n",
       "      <td>1.92173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1881</td>\n",
       "      <td>1885.0</td>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.24649</td>\n",
       "      <td>1.74091</td>\n",
       "      <td>0.58331</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-1.51840</td>\n",
       "      <td>0.88113</td>\n",
       "      <td>0.76540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1882</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>-0.07854</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>0.45468</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>1.13281</td>\n",
       "      <td>-1.37639</td>\n",
       "      <td>-1.27553</td>\n",
       "      <td>-1.77200</td>\n",
       "      <td>-1.38502</td>\n",
       "      <td>0.52975</td>\n",
       "      <td>-0.52593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1883</td>\n",
       "      <td>1887.0</td>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>0.91093</td>\n",
       "      <td>-1.92173</td>\n",
       "      <td>0.29338</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-2.57309</td>\n",
       "      <td>1.29221</td>\n",
       "      <td>1.22470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1884</td>\n",
       "      <td>1888.0</td>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>0.21128</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>2.12700</td>\n",
       "      <td>1.65653</td>\n",
       "      <td>1.11406</td>\n",
       "      <td>0.41594</td>\n",
       "      <td>0.88113</td>\n",
       "      <td>1.22470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1885 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      Age   Gender  Education  Country  Ethnicity  Neuroticism  \\\n",
       "0        1.0  0.49788  0.48246   -0.05921  0.96082    0.12600      0.31287   \n",
       "1        2.0 -0.07854 -0.48246    1.98437  0.96082   -0.31685     -0.67825   \n",
       "2        3.0  0.49788 -0.48246   -0.05921  0.96082   -0.31685     -0.46725   \n",
       "3        4.0 -0.95197  0.48246    1.16365  0.96082   -0.31685     -0.14882   \n",
       "4        5.0  0.49788  0.48246    1.98437  0.96082   -0.31685      0.73545   \n",
       "...      ...      ...      ...        ...      ...        ...          ...   \n",
       "1880  1884.0 -0.95197  0.48246   -0.61113 -0.57009   -0.31685     -1.19430   \n",
       "1881  1885.0 -0.95197 -0.48246   -0.61113 -0.57009   -0.31685     -0.24649   \n",
       "1882  1886.0 -0.07854  0.48246    0.45468 -0.57009   -0.31685      1.13281   \n",
       "1883  1887.0 -0.95197  0.48246   -0.61113 -0.57009   -0.31685      0.91093   \n",
       "1884  1888.0 -0.95197 -0.48246   -0.61113  0.21128   -0.31685     -0.46725   \n",
       "\n",
       "      Extraversion  Openness  Agreeableness  Conscientiousness  Impulsive  \\\n",
       "0         -0.57545  -0.58331       -0.91699           -0.00665   -0.21712   \n",
       "1          1.93886   1.43533        0.76096           -0.14277   -0.71126   \n",
       "2          0.80523  -0.84732       -1.62090           -1.01450   -1.37983   \n",
       "3         -0.80615  -0.01928        0.59042            0.58489   -1.37983   \n",
       "4         -1.63340  -0.45174       -0.30172            1.30612   -0.21712   \n",
       "...            ...       ...            ...                ...        ...   \n",
       "1880       1.74091   1.88511        0.76096           -1.13788    0.88113   \n",
       "1881       1.74091   0.58331        0.76096           -1.51840    0.88113   \n",
       "1882      -1.37639  -1.27553       -1.77200           -1.38502    0.52975   \n",
       "1883      -1.92173   0.29338       -1.62090           -2.57309    1.29221   \n",
       "1884       2.12700   1.65653        1.11406            0.41594    0.88113   \n",
       "\n",
       "      Sensation Seeking  Benzos  Caffeine  Cocaine  LSD  Heroin  \n",
       "0              -1.18084     0.0       1.0      0.0  0.0     0.0  \n",
       "1              -0.21575     0.0       1.0      1.0  0.0     0.0  \n",
       "2               0.40148     0.0       1.0      0.0  0.0     0.0  \n",
       "3              -1.18084     1.0       1.0      0.0  0.0     0.0  \n",
       "4              -0.21575     0.0       1.0      0.0  0.0     0.0  \n",
       "...                 ...     ...       ...      ...  ...     ...  \n",
       "1880            1.92173     0.0       1.0      0.0  1.0     0.0  \n",
       "1881            0.76540     0.0       1.0      0.0  1.0     0.0  \n",
       "1882           -0.52593     1.0       1.0      1.0  0.0     0.0  \n",
       "1883            1.22470     0.0       1.0      0.0  1.0     0.0  \n",
       "1884            1.22470     1.0       1.0      1.0  1.0     0.0  \n",
       "\n",
       "[1885 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"drugConsumption_5Response.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "drugs = [\"Benzos\", \"Caffeine\",\"Cocaine\",\"LSD\",\"Heroin\"]\n",
    "X = data[['Age','Gender','Education', 'Country', 'Ethnicity', 'Neuroticism', 'Extraversion', 'Openness', 'Agreeableness', 'Conscientiousness', 'Impulsive', 'Sensation Seeking']]\n",
    "y_testall = np.zeros((472,5))\n",
    "yhat_all = np.zeros((472,5))\n",
    "yhat_prob = np.zeros((472,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[288,  42],\n",
       "       [ 86,  56]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data.Benzos, test_size = 0.25, random_state=42)\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "sb = model.score(X_test, y_test)\n",
    "#sklm.roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
    "sklm.confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_all[:,0] = pred\n",
    "y_testall[:,0] = y_test\n",
    "yhat_prob[:,0] = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  16],\n",
       "       [  0, 456]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, data.Caffeine, test_size = 0.25, random_state=42)\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "sC = model.score(X_test, y_test)\n",
    "sklm.confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_all[:,1] = pred\n",
    "y_testall[:,1] = y_test\n",
    "yhat_prob[:,1] = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[350,  16],\n",
       "       [ 87,  19]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, data.Cocaine, test_size = 0.25, random_state=42)\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "sCoc = model.score(X_test, y_test)\n",
    "sklm.confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_all[:,2] = pred\n",
    "y_testall[:,2] = y_test\n",
    "yhat_prob[:,2] = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[331,  42],\n",
       "       [ 50,  49]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, data.LSD, test_size = 0.25,random_state=42)\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "sL = model.score(X_test, y_test)\n",
    "sklm.confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_all[:,3] = pred\n",
    "y_testall[:,3] = y_test\n",
    "yhat_prob[:,3] = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[438,   0],\n",
       "       [ 34,   0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, data.Heroin, test_size = 0.25,random_state=42)\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "sH = model.score(X_test, y_test)\n",
    "sklm.confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_all[:,4] = pred\n",
    "y_testall[:,4] = y_test\n",
    "yhat_prob[:,4] = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming = sklm.hamming_loss(y_testall,yhat_all)\n",
    "subsetacc = sklm.accuracy_score(y_testall,yhat_all)\n",
    "aucA = sklm.roc_auc_score(y_testall,yhat_prob,average=None)\n",
    "auc = sklm.roc_auc_score(y_testall,yhat_prob,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15805084745762712\n",
      "0.5127118644067796\n",
      "0.7514228488259718\n",
      "[0.76564234 0.70847039 0.80546964 0.83784223 0.84797207]\n"
     ]
    }
   ],
   "source": [
    "print(hamming)\n",
    "print(subsetacc)\n",
    "print(auc)\n",
    "print(aucA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[288  42]\n",
      "  [ 86  56]]\n",
      "\n",
      " [[  0  16]\n",
      "  [  0 456]]\n",
      "\n",
      " [[350  16]\n",
      "  [ 87  19]]\n",
      "\n",
      " [[331  42]\n",
      "  [ 50  49]]\n",
      "\n",
      " [[438   0]\n",
      "  [ 34   0]]]\n"
     ]
    }
   ],
   "source": [
    "mcm = sklm.multilabel_confusion_matrix(y_testall,yhat_all)\n",
    "print(mcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3943662  1.         0.17924528 0.49494949 0.        ]\n"
     ]
    }
   ],
   "source": [
    "recall = tp / (tp + fn)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87272727 0.         0.95628415 0.88739946 1.        ]\n"
     ]
    }
   ],
   "source": [
    "specificity = tn / (tn + fp)\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
